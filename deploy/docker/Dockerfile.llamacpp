# Build llama.cpp on Alpine
FROM alpine:3.22 AS builder

# Install build dependencies
RUN apk add --no-cache \
    git \
    build-base \
    cmake \
    linux-headers \
    openblas-dev \
    curl-dev

# Clone and build llama.cpp
WORKDIR /build
RUN git clone https://github.com/ggerganov/llama.cpp.git
WORKDIR /build/llama.cpp

# Build with OpenBLAS support (static build)
RUN mkdir build && \
    cd build && \
    cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS -DLLAMA_CURL=OFF -DLLAMA_STATIC=ON && \
    cmake --build . --config Release

# Final stage
FROM alpine:3.22

# Install runtime dependencies
RUN apk add --no-cache \
    libstdc++ \
    libgomp \
    openblas \
    && rm -rf /var/cache/apk/*

# Copy binary and shared libraries
COPY --from=builder /build/llama.cpp/build/bin/llama-server /usr/local/bin/llama-server
COPY --from=builder /build/llama.cpp/build/bin/*.so /usr/local/lib/

# Copy wrapper script and ensure it has proper line endings
COPY deploy/docker/llama-wrapper.sh /usr/local/bin/llama-wrapper.sh
RUN chmod +x /usr/local/bin/llama-wrapper.sh && \
    dos2unix /usr/local/bin/llama-wrapper.sh 2>/dev/null || true

# Create model directory
RUN mkdir -p /models

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# Default command
ENTRYPOINT ["/usr/local/bin/llama-wrapper.sh"]
CMD ["--host", "0.0.0.0", \
     "--port", "8080", \
     "--n-gpu-layers", "-1", \
     "--ctx-size", "8192", \
     "--parallel", "4"]