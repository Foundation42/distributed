# Build llama.cpp on Alpine
FROM alpine:3.19 AS builder

# Install build dependencies
RUN apk add --no-cache \
    git \
    build-base \
    cmake \
    linux-headers \
    openblas-dev

# Clone and build llama.cpp
WORKDIR /build
RUN git clone https://github.com/ggerganov/llama.cpp.git
WORKDIR /build/llama.cpp

# Build with OpenBLAS support
RUN mkdir build && \
    cd build && \
    cmake .. -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS && \
    cmake --build . --config Release

# Final stage
FROM alpine:3.19

# Install runtime dependencies
RUN apk add --no-cache \
    libstdc++ \
    libgomp \
    openblas \
    && rm -rf /var/cache/apk/*

# Copy binaries
COPY --from=builder /build/llama.cpp/build/bin/server /usr/local/bin/llama-server

# Create model directory
RUN mkdir -p /models

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# Default command
ENTRYPOINT ["/usr/local/bin/llama-server"]
CMD ["--host", "0.0.0.0", \
     "--port", "8080", \
     "--n-gpu-layers", "-1", \
     "--ctx-size", "8192", \
     "--parallel", "4"]